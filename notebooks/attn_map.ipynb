{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from os import getenv\n",
    "\n",
    "import cv2\n",
    "import nopdb\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from spoof.dataset.dataset import FaceDataset\n",
    "from spoof.model.vit import ViT\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and visualize an image returned from the ```FaceDataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataset to get a random image and read it\n",
    "os.chdir(getenv(\"HOME\") + \"/mipt/spoof\")\n",
    "ds = FaceDataset(\"data/casia/test/annotations.csv\")\n",
    "img = ds[np.random.randint(len(ds))][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions to map the tensor back to an image\n",
    "def inv_normalize(tensor):\n",
    "    \"\"\"Normalize an image tensor back to the 0-255 range.\"\"\"\n",
    "    tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min()) * (256 - 1e-5)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def inv_transform(tensor, normalize=True):\n",
    "    \"\"\"Convert a tensor to an image.\"\"\"\n",
    "    if normalize:\n",
    "        tensor = inv_normalize(tensor)\n",
    "    array = tensor.detach().cpu().numpy()\n",
    "    array = array.transpose(1, 2, 0).astype(np.uint8)\n",
    "    return PIL.Image.fromarray(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_transform(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and let it load the weights in the constructor\n",
    "model = ViT()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capture attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_attn_call(input, layer_idx):\n",
    "    with nopdb.capture_call(\n",
    "        model.extractor.encoder.layers[layer_idx].self_attention.forward\n",
    "    ) as attn_call:\n",
    "        with torch.no_grad():\n",
    "            model(input)\n",
    "    return attn_call\n",
    "\n",
    "\n",
    "# Get the attention matrix for the last layer\n",
    "attn_matrix = capture_attn_call(img, 11).locals[\"attn_output_weights\"][0]\n",
    "print(f\"Attention matrix shape: {attn_matrix.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize attention map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_map(img, layer_idx, get_mask=False):\n",
    "    \"\"\"Get the attention map for an image.\"\"\"\n",
    "    attn_matrix = capture_attn_call(img, layer_idx).locals[\"attn_output_weights\"]\n",
    "\n",
    "    # Average the attention weights across all heads.\n",
    "    attn_matrix = torch.mean(attn_matrix, dim=1)\n",
    "\n",
    "    # To account for residual connections, we add an identity matrix to the\n",
    "    # attention matrix and re-normalize the weights.\n",
    "    residual_att = torch.eye(attn_matrix.size(1))\n",
    "    aug_att_mat = attn_matrix + residual_att\n",
    "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "    # Recursively multiply the weight matrices\n",
    "    joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "    joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "    for n in range(1, aug_att_mat.size(0)):\n",
    "        joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n - 1])\n",
    "\n",
    "    v = joint_attentions[-1]\n",
    "    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "    mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
    "\n",
    "    if get_mask:\n",
    "        result = cv2.resize(mask / mask.max(), (img.shape[1], img.shape[2]))\n",
    "    else:\n",
    "        mask = cv2.resize(mask / mask.max(), (img.shape[1], img.shape[2]))[..., np.newaxis]\n",
    "        result = mask * img.numpy().transpose(1, 2, 0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(img, save_path=None):\n",
    "    \"\"\"Plot the attention maps for each layer.\"\"\"\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "\n",
    "    # Show the attention maps for each layer with mask\n",
    "    for i in range(12):\n",
    "        ax = fig.add_subplot(4, 3, i + 1)\n",
    "        ax.imshow(get_attention_map(img, i))\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Layer {i + 1}\")\n",
    "        fig.tight_layout()\n",
    "        fig.suptitle(\"Attention Maps, without mask\", fontsize=16)\n",
    "\n",
    "    # Show the attention maps for each layer with mask on the same figure\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    for i in range(12):\n",
    "        ax = fig.add_subplot(4, 3, i + 1)\n",
    "        ax.imshow(get_attention_map(img, i, get_mask=True))\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Layer {i + 1}\")\n",
    "        fig.tight_layout()\n",
    "        fig.suptitle(\"Attention Maps, with mask\", fontsize=16)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize attention map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a couple of attention maps for Casia train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "split = [\"train\", \"test\"]\n",
    "\n",
    "# create subfolders for each split\n",
    "os.makedirs(\"figures/attention/casia/train\", exist_ok=True)\n",
    "os.makedirs(\"figures/attention/casia/test\", exist_ok=True)\n",
    "\n",
    "for s in split:\n",
    "    casia = FaceDataset(f\"data/casia/{s}/annotations.csv\")\n",
    "    for i in range(num_samples):\n",
    "        idx = np.random.randint(len(casia))\n",
    "        img = casia[idx][\"image\"]\n",
    "        fn = casia[idx][\"filename\"].split(\"/\")[-1]\n",
    "        folder = f\"figures/attention/casia/{s}\"\n",
    "        save_path = f\"{folder}/{fn}\"\n",
    "        plot_attention(img, save_path=save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind = [\"real\", \"attack\"]\n",
    "\n",
    "# create subfolders for each split\n",
    "for k in kind:\n",
    "    os.makedirs(f\"figures/attention/replay/train/{k}\", exist_ok=True)\n",
    "    os.makedirs(f\"figures/attention/replay/test/{k}\", exist_ok=True)\n",
    "\n",
    "for s in split:\n",
    "    replay = FaceDataset(f\"data/replay/{s}/annotations.csv\")\n",
    "    for k in kind:\n",
    "        for i in range(num_samples):\n",
    "            idx = np.random.randint(len(replay))\n",
    "            img = replay[idx][\"image\"]\n",
    "            fn = replay[idx][\"filename\"].split(\"/\")[-1]\n",
    "            save_path = f\"figures/attention/replay/{s}/{k}/{fn}\"\n",
    "            plot_attention(img, save_path=save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
