{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from os import getenv\n",
    "\n",
    "import cv2\n",
    "import nopdb\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from spoof.dataset.dataset import FaceDataset\n",
    "from spoof.model.vit import ViT\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(getenv(\"HOME\") + \"/mipt/spoof\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions to map the tensor back to an image\n",
    "def inv_normalize(tensor):\n",
    "    \"\"\"Normalize an image tensor back to the 0-255 range.\"\"\"\n",
    "    tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min()) * (256 - 1e-5)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def inv_transform(tensor, normalize=True):\n",
    "    \"\"\"Convert a tensor to an image.\"\"\"\n",
    "    if normalize:\n",
    "        tensor = inv_normalize(tensor)\n",
    "    array = tensor.detach().cpu().numpy()\n",
    "    array = array.transpose(1, 2, 0).astype(np.uint8)\n",
    "    return PIL.Image.fromarray(array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweaking the vision transformer source code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here I load the model from our `ViT` class. To be able to capture the attention weights following modifications are made to the vision transformer source code:\n",
    "\n",
    "- **`torchvision.models.vision_transformer.py`**, line `113`:\n",
    "\n",
    "```\n",
    "    class EncoderBlock(nn.Module):\n",
    "    ...\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        ...\n",
    "        x, _ = self.self_attention(x, x, x, need_weights=True) # We need attention weights\n",
    "        ...\n",
    "```\n",
    "\n",
    "- **`torch.nn.modules.activation.py`**, line `1026`:\n",
    "\n",
    "```\n",
    "    class MultiheadAttention(nn.Module):\n",
    "    ...\n",
    "    def forward(\n",
    "            ...\n",
    "            average_attn_weights: bool = False, # Easier if set to False\n",
    "            ...\n",
    "        ):\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and let it load the weights in the constructor\n",
    "model = ViT()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capture attention weights using ```nopdb``` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_attn_call(input, layer_idx):\n",
    "    with nopdb.capture_call(\n",
    "        model.extractor.encoder.layers[layer_idx].self_attention.forward\n",
    "    ) as attn_call:\n",
    "        with torch.no_grad():\n",
    "            model(input)\n",
    "    return attn_call\n",
    "\n",
    "\n",
    "# Get the attention matrix for the last layer for an image returned from from the dataset iterator\n",
    "img = next(iter(FaceDataset(\"data/casia/train/annotations.csv\")))[\"image\"]\n",
    "attn_matrix = capture_attn_call(img, 11).locals[\"attn_output_weights\"][0]\n",
    "print(f\"Attention matrix shape: {attn_matrix.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize attention map "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I modified this function from the kaggle link you sent me, I'm not 100 % clear what's going on here but it worked after modifying a bit.\n",
    "\n",
    "- Other visualization implementations didn't work quite well for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_map(img, layer_idx, get_mask=False):\n",
    "    \"\"\"Get the attention map for an image.\"\"\"\n",
    "    attn_matrix = capture_attn_call(img, layer_idx).locals[\"attn_output_weights\"]\n",
    "\n",
    "    # Average the attention weights across all heads.\n",
    "    attn_matrix = torch.mean(attn_matrix, dim=1)\n",
    "\n",
    "    # To account for residual connections, we add an identity matrix to the\n",
    "    # attention matrix and re-normalize the weights.\n",
    "    residual_att = torch.eye(attn_matrix.size(1))\n",
    "    aug_att_mat = attn_matrix + residual_att\n",
    "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "    # Recursively multiply the weight matrices\n",
    "    joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "    joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "    for n in range(1, aug_att_mat.size(0)):\n",
    "        joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n - 1])\n",
    "\n",
    "    v = joint_attentions[-1]\n",
    "    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "    mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
    "\n",
    "    if get_mask:\n",
    "        result = cv2.resize(mask / mask.max(), (img.shape[1], img.shape[2]))\n",
    "    else:\n",
    "        mask = cv2.resize(mask / mask.max(), (img.shape[1], img.shape[2]))[..., np.newaxis]\n",
    "        result = mask * img.numpy().transpose(1, 2, 0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(img, save_path=None):\n",
    "    \"\"\"Plot the attention maps for each layer.\"\"\"\n",
    "    # Get the attention maps for each layer with and without mask and combine them into a single list\n",
    "    attns_nomask = [get_attention_map(img, i) for i in range(12)]\n",
    "    attns_mask = [get_attention_map(img, i, get_mask=True) for i in range(12)]\n",
    "    attns = [None] * (len(attns_nomask) + len(attns_mask))\n",
    "    attns[::2] = attns_nomask\n",
    "    attns[1::2] = attns_mask\n",
    "    fn = os.path.basename(save_path)\n",
    "    # Plot the attention maps for each layer with and without mask side by side\n",
    "    fig = plt.figure(figsize=(32, 32))\n",
    "    for i, attn in enumerate(attns):\n",
    "        ax = fig.add_subplot(4, 6, i + 1)\n",
    "        ax.imshow(attn)\n",
    "        ax.axis(\"off\")\n",
    "        if i % 2 == 0:\n",
    "            ax.set_title(f\"Layer {i // 2} (w/o mask)\", fontsize=24)\n",
    "        else:\n",
    "            ax.set_title(f\"Layer {i // 2} (w/ mask)\", fontsize=24)\n",
    "        fig.tight_layout()\n",
    "        fig.suptitle(f\"Attention Maps for {fn}\", fontsize=32)\n",
    "\n",
    "    # Save\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "        plt.close(\"all\")\n",
    "    else:\n",
    "        fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize attention map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(img, save_path=\"attn_map.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a couple of attention maps for Casia & Replay train and test splits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I preserved the folder structure of what I have locally for both datasets and saved a number of above figures for you to examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "split = [\"train\", \"test\"]\n",
    "\n",
    "# create subfolders for each split, sorry if paths are not Windows compatible, I will fix it later\n",
    "os.makedirs(\"figures/attention/casia/train\", exist_ok=True)\n",
    "os.makedirs(\"figures/attention/casia/test\", exist_ok=True)\n",
    "\n",
    "for s in split:\n",
    "    casia = FaceDataset(f\"data/casia/{s}/annotations.csv\")\n",
    "    for i in range(num_samples):\n",
    "        idx = np.random.randint(len(casia))\n",
    "        img = casia[idx][\"image\"]\n",
    "        fn = casia[idx][\"filename\"].split(\"/\")[-1]\n",
    "        folder = f\"figures/attention/casia/{s}\"\n",
    "        save_path = f\"{folder}/{fn}\"\n",
    "        plot_attention(img, save_path=save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kind = [\"real\", \"attack\"]\n",
    "\n",
    "# # create subfolders for each split\n",
    "# for k in kind:\n",
    "#     os.makedirs(f\"figures/attention/replay/train/{k}\", exist_ok=True)\n",
    "#     os.makedirs(f\"figures/attention/replay/test/{k}\", exist_ok=True)\n",
    "\n",
    "# for s in split:\n",
    "#     replay = FaceDataset(f\"data/replay/{s}/annotations.csv\")\n",
    "#     for k in kind:\n",
    "#         for i in range(num_samples):\n",
    "#             idx = np.random.randint(len(replay))\n",
    "#             img = replay[idx][\"image\"]\n",
    "#             fn = replay[idx][\"filename\"].split(\"/\")[-1]\n",
    "#             save_path = f\"figures/attention/replay/{s}/{k}/{fn}\"\n",
    "#             plot_attention(img, save_path=save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
