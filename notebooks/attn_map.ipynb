{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from os import getenv\n",
    "\n",
    "import cv2\n",
    "import nopdb\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from spoof.dataset.dataset import FaceDataset\n",
    "from spoof.model.vit import ViT\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and visualize an image returned from the ```FaceDataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataset to get a random image and read it\n",
    "os.chdir(getenv(\"HOME\") + \"/mipt/spoof\")\n",
    "ds = FaceDataset(\"data/casia/test/annotations.csv\")\n",
    "img = ds[np.random.randint(len(ds))][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions to map the tensor back to an image\n",
    "def inv_normalize(tensor):\n",
    "    \"\"\"Normalize an image tensor back to the 0-255 range.\"\"\"\n",
    "    tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min()) * (256 - 1e-5)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def inv_transform(tensor, normalize=True):\n",
    "    \"\"\"Convert a tensor to an image.\"\"\"\n",
    "    if normalize:\n",
    "        tensor = inv_normalize(tensor)\n",
    "    array = tensor.detach().cpu().numpy()\n",
    "    array = array.transpose(1, 2, 0).astype(np.uint8)\n",
    "    return PIL.Image.fromarray(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_transform(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and let it load the weights in the constructor\n",
    "model = ViT()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capture attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(input):\n",
    "#     \"\"\"Run the model on an input and print the predicted classes with probabilities.\"\"\"\n",
    "#     with torch.no_grad():\n",
    "#         return model.get_liveness_score(model(input))\n",
    "\n",
    "\n",
    "def capture_attn_call(input, layer_idx):\n",
    "    with nopdb.capture_call(\n",
    "        model.extractor.encoder.layers[layer_idx].self_attention.forward\n",
    "    ) as attn_call:\n",
    "        with torch.no_grad():\n",
    "            model(input)\n",
    "    return attn_call\n",
    "\n",
    "\n",
    "# Get the attention matrix for the last layer\n",
    "attn_matrix = capture_attn_call(img, 11).locals[\"attn_output_weights\"][0]\n",
    "print(f\"Attention matrix shape: {attn_matrix.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize attention map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_map(img, layer_idx, get_mask=False):\n",
    "    \"\"\"Get the attention map for an image.\"\"\"\n",
    "    attn_matrix = capture_attn_call(img, layer_idx).locals[\"attn_output_weights\"]\n",
    "\n",
    "    # Average the attention weights across all heads.\n",
    "    attn_matrix = torch.mean(attn_matrix, dim=1)\n",
    "\n",
    "    # To account for residual connections, we add an identity matrix to the\n",
    "    # attention matrix and re-normalize the weights.\n",
    "    residual_att = torch.eye(attn_matrix.size(1))\n",
    "    aug_att_mat = attn_matrix + residual_att\n",
    "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "    # Recursively multiply the weight matrices\n",
    "    joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "    joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "    for n in range(1, aug_att_mat.size(0)):\n",
    "        joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n - 1])\n",
    "\n",
    "    v = joint_attentions[-1]\n",
    "    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "    mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
    "\n",
    "    if get_mask:\n",
    "        result = cv2.resize(mask / mask.max(), (img.shape[1], img.shape[2]))\n",
    "    else:\n",
    "        mask = cv2.resize(mask / mask.max(), (img.shape[1], img.shape[2]))[..., np.newaxis]\n",
    "        # print(f\"MASK SHAPE: {mask.shape}\")\n",
    "        # print(f\"IMG TRANSPOSE SHAPE: {img.numpy().transpose(1, 2, 0).shape}\")\n",
    "        # print(f\"IMG TRANSPOSE: {img.numpy().transpose(1, 2, 0)[0, :10, :1]}\")\n",
    "        # print(f\"AFTER MASK: {mask * img.numpy().transpose(1, 2, 0)}\")\n",
    "        # print(f\"AFTER UINT8: {(mask * img.numpy().transpose(1, 2, 0)).astype('uint8')}\")\n",
    "\n",
    "        result = mask * img.numpy().transpose(1, 2, 0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attention(img, get_mask=False):\n",
    "    # Plot the attention maps for each layer without mask\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    fig.suptitle(f\"Attention Maps, mask={get_mask}\", fontsize=12)\n",
    "    for i in range(12):\n",
    "        ax = plt.subplot(3, 4, i + 1)\n",
    "        attn_map = get_attention_map(img, layer_idx=i, get_mask=get_mask)\n",
    "        ax.set_title(f\"Layer {i + 1}\")\n",
    "        ax.imshow(attn_map)\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(f\"attn_map_mask={get_mask}.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_attention(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize with mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the attention maps for each layer with mask\n",
    "show_attention(img, get_mask=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
