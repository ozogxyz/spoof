{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "\n",
    "import cv2\n",
    "import IPython.display as ipd\n",
    "import nopdb\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from spoof.dataset.dataset import FaceDataset\n",
    "from spoof.model.vit import ViT\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and visualize an image returned from the ```FaceDataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataset to get a random image and read it\n",
    "os.chdir(\"..\")\n",
    "ds = FaceDataset(\"data/casia/test/annotations.csv\")\n",
    "img = ds[np.random.randint(len(ds))][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions to map the tensor back to an image\n",
    "def inv_normalize(tensor):\n",
    "    \"\"\"Normalize an image tensor back to the 0-255 range.\"\"\"\n",
    "    tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min()) * (256 - 1e-5)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def inv_transform(tensor, normalize=True):\n",
    "    \"\"\"Convert a tensor to an image.\"\"\"\n",
    "    if normalize:\n",
    "        tensor = inv_normalize(tensor)\n",
    "    array = tensor.detach().cpu().numpy()\n",
    "    array = array.transpose(1, 2, 0).astype(np.uint8)\n",
    "    return array\n",
    "\n",
    "\n",
    "def show_img(tensor):\n",
    "    array = inv_transform(tensor)\n",
    "    return PIL.Image.fromarray(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and let it load the weights in the constructor\n",
    "model = ViT()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capture attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input):\n",
    "    \"\"\"Run the model on an input and print the predicted classes with probabilities.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        score = model.get_liveness_score(model(input))\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_attn_call(input, layer_idx):\n",
    "    with nopdb.capture_call(\n",
    "        model.extractor.encoder.layers[layer_idx].self_attention.forward\n",
    "    ) as attn_call:\n",
    "        predict(input).item()\n",
    "    return attn_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the attention matrix for the last layer\n",
    "attn_mat = get_attn_call(img, 11).locals[\"attn_output_weights\"]\n",
    "print(f\"Attention matrix shape: {attn_mat.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize attention map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_map(img, layer_idx, get_mask=False):\n",
    "    \"\"\"Get the attention map for an image.\"\"\"\n",
    "    attn_mat = get_attn_call(img, layer_idx).locals[\"attn_output_weights\"]\n",
    "\n",
    "    # Average the attention weights across all heads.\n",
    "    att_mat = torch.mean(attn_mat, dim=1)\n",
    "\n",
    "    # To account for residual connections, we add an identity matrix to the\n",
    "    # attention matrix and re-normalize the weights.\n",
    "    residual_att = torch.eye(att_mat.size(1))\n",
    "    aug_att_mat = att_mat + residual_att\n",
    "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "    # Recursively multiply the weight matrices\n",
    "    joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "    joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "    for n in range(1, aug_att_mat.size(0)):\n",
    "        joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n - 1])\n",
    "\n",
    "    v = joint_attentions[-1]\n",
    "    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "    mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
    "\n",
    "    if get_mask:\n",
    "        result = cv2.resize(mask / mask.max(), (img.shape[1], img.shape[2]))\n",
    "    else:\n",
    "        # result = cv2.resize(img.numpy().transpose(1, 2, 0), (img.shape[1], img.shape[2]))\n",
    "        # result = result.astype(np.float32) + 1\n",
    "        # result /= result.max()\n",
    "        mask = cv2.resize(mask / mask.max(), (img.shape[1], img.shape[2]))[..., np.newaxis]\n",
    "        result = (mask * img.numpy().transpose(1, 2, 0)).astype(\"uint8\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_map(original_img, att_map):\n",
    "    _, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "    ax1.set_title(\"Original\")\n",
    "    ax2.set_title(\"Attention Map Last Layer\")\n",
    "    _ = ax1.imshow(original_img)\n",
    "    _ = ax2.imshow(att_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_map = get_attention_map(img, layer_idx=11, get_mask=False)\n",
    "original_img = inv_transform(img)\n",
    "plot_attention_map(original_img, att_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize with mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_map = get_attention_map(img, layer_idx=11, get_mask=True)\n",
    "original_img = inv_transform(img)\n",
    "plot_attention_map(original_img, att_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
